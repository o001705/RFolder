{
    "collab_server" : "",
    "contents" : "taex1 <- function(book_index) {\n  #############################################################################\n  # Exercise 1\n  # Create a vector of statements. In order to do that, use those statements\n  # of the introduction of the r-exercise “Data Science for Operational\n  # Excellence (Part-5)”.\n  #############################################################################\n\n  text <- c(\"Operations need to have demand forecasts in order to establish optimal resource allocation policies. \",\n            \"But, when we make predictions the only thing that we assure is the occurrence of prediction errors. \",\n            \"Fortunately, there is no need to be 100% accurate to succeed, we just need to perform better than our competitors. \",\n            \"In this exercise we will learn a practical approach to predict using the forecast package. \")\n  text\n\n\n  #############################################################################\n  # Exercise 2\n  # Transform this vector into a data frame. Create a first column called Item\n  # to put a sequence in those statements.\n  #############################################################################\nlibrary(dplyr)\n  list_len <- length(text)\n  text_df <- data_frame(line = 1:list_len, text = text)\n  text_df\n\n  #############################################################################\n  # Exercise 3\n  # How many times the string predict appears?\n  #############################################################################\n  lines <- 0\n  for(i in 1:list_len){\n    lines <- lines + grepl(\"[p][r][e][d][i][c][t]\", text_df[i,2])\n  }\n  lines\n\n\n  #############################################################################\n  # Exercise 4\n  # What are the sentences where predict appears?\n  #############################################################################\n  place <- vector()\n  for(i in 1:list_len){\n    place[i] <- grepl(\"[p][r][e][d][i][c][t]\", text_df[i,2])\n  }\n  text_df[place,2]\n\n\n  #############################################################################\n  # Exercise 5\n  # In witch Items does forecast appears?\n  #############################################################################\n  for(i in 1:list_len){\n    place[i] <- grepl(\"[p][r][e][d][i][c][t]\", text_df[i,2])\n  }\n  text_df[place,1]\n\n\n  #############################################################################\n  # Exercise 6\n  # Count the number of times the word predict appears. Count the number of\n  # times the word forecast appears. Calculate the proportions of predict/\n  # forecast on this text.\n  #############################################################################\n  word1 <- 0\n  word2 <- 0\n  for(i in 1:list_len){\n    word1 <- word1 + grepl(\"[p][r][e][d][i][c][t]\", text_df[i,2])\n    word2 <- word2 + grepl(\"[f][o][r][e][c][a][s][t]\", text_df[i,2])\n  }\n  word1/word2\n\n\n  #############################################################################\n  # Exercise 7\n  # Break those sentences in words and create a new data frame that the word\n  # that repeats the most appears in the first line.\n  #############################################################################\n library(tidytext)\n  text_df %>%\n    unnest_tokens(word, text)%>%\n    count(word, sort = TRUE)\n\n  #############################################################################\n  # Exercise 8\n  # As you can see, there are many words that are irrelevant, like “to”, “the”.\n  # Take this words off using data(stop_words).\n  #############################################################################\n  data(stop_words)\n  text_df %>%\n    unnest_tokens(word, text)%>%\n    count(word, sort = TRUE)%>%\n    anti_join(stop_words)\n\n\n  #############################################################################\n  # Exercise 9\n  # Download the book 768 from the library(gutenbergr).\n  #############################################################################\n library(gutenbergr)\n  book <- gutenberg_download(as.numeric(book_index))\n\n  print((gutenberg_metadata %>%\n    filter(as.numeric(gutenberg_id) == as.numeric(book_index)))[\"title\"])\n\n  #############################################################################\n  # Exercise 10\n  # Please, ignore the stopping words, and find out the words that appear\n  # in descending order.\n  #############################################################################\n\n  book %>%\n    unnest_tokens(word, text)%>%\n    count(word, sort = TRUE)%>%\n    anti_join(stop_words)\n\n  book$linenumber <- seq.int(nrow(book))\n\n  library(tidyr)\n  library(stringr)\n\n  book_sentiment <- book %>%\n    unnest_tokens(word, text)%>%\n    inner_join(get_sentiments(\"bing\")) %>%\n    count(word,index = linenumber %/% 200,sentiment) %>%\n    spread(sentiment, n, fill = 0) %>%\n    mutate(sentiment = positive - negative)\n\n  library(ggplot2)\n\n  ggplot(book_sentiment, aes(index, sentiment)) +\n    geom_bar(stat=\"identity\", fill=\"#FF9999\", position=position_dodge())\n\n}\n",
    "created" : 1496194862953.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2949298640",
    "id" : "9CB927C4",
    "lastKnownWriteTime" : 1496300343,
    "last_content_update" : 1496300343320,
    "path" : "~/RFolder/TextAnalytics1/R/TextAnalyticsEx1.R",
    "project_path" : "R/TextAnalyticsEx1.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}